1. Study the file structure of the project properly.

TaskFlow
| ── backend/
    │── app/
    │   ├── main.py              # Entry point of FastAPI app
    │   ├── database.py          # Handles DB connections
    │   ├── models.py            # SQLAlchemy models (PostgreSQL)
    │   ├── crud.py              # Database operations (CRUD)
    │   ├── routes/
    │   │   ├── __init__.py      # Makes it a package
    │   │   ├── user.py          # Example: User-related routes
    │   │   ├── task.py          # Example: Task-related routes
    │   ├── schemas.py           # Pydantic models for request validation
    │   ├── config.py            # Settings & environment variables
    │   ├── redis.py             # Redis connection setup
    │── requirements.txt
    │── backend.dockerfile
    │── ...
| ── frontend/
    │── src/
    │── backend.dockerfile
    │── ...
| ── .env
| ── .env.db
| ── .gitignore
| ── docker-compose.yml
| ── README.md
__________________________________________________________________________________________________________

2. Study the contents and comments in backend/backend.dockerfile in detail.
__________________________________________________________________________________________________________

3. NOTES ON backend.dockerfile:

  Notice that we copy the requirements.txt first and then the rest of the contents into /app
  - If we copy everything first and then install the dependencies such as:
    COPY . .   # Copies ALL files, including app code
    RUN pip install --no-cache-dir -r requirements.txt

  If we make a small code change in our FastAPI app, Docker rebuilds everything from scratch, including reinstalling dependencies.
  A better approach would be to:
  - Copy only requirements.txt first.
  - Install dependencies.
  - Then copy the rest of the files.

  To build the image, we use the command:
  - "docker build -t taskflow-backend -f backend.dockerfile ."
  - the -t flag	names (tags) the built image
  - the -f flag specifies a custom Dockerfile
  - It searches for the dockerfile within '.' path
__________________________________________________________________________________________________________

4. NOTES ON docker-compose.yml [IMPORTANT]:

  Notice that version 3.8 specifies that we are using Docker Compose file format version 3.8
  
  We define all the services we need: backend, postgresql, mongodb, redis, and frontend
  - Notice that services like backend and frontend, we need to build them because we have our custom dockerfile for each of them that (for backend):
    - copies all the necessary items into the container image
    - installs all the necessary dependencies prior
    - runs the FastAPI backend api
  - In contrast, we use simple images for postgresql, mongodb, and redis provided by docker

  Notice all the fields within a service
    - build uses context for the location of the dockerfile and the dockerfile itself

    - ports: "8000:8000" mean that for us (local machine), we can access the backend at: http://localhost:8000,
      where our computer is the localhost. But, for the container, the same backend service is running at: http://localhost:8000 where the container is the localhost
      Similarly, to take an example, we can access the postgresql instance at http://localhost:5432
      and the container has the same postgresql instance running at http://localhost:5432

    - depends_on lists the services our backend is dependent upon and starts those services prior to the backend
    - condition ensures that the services are either healthy, or started before starting the backend service
    - COME BACK TO THIS LATER!

    - study the environment variables for each of the service for info on the instance running

    - healthcheck ensures postgresql, and mongodb are working and responsive (every 5 seconds) starting from 
      10 seconds after launch, and retries upto 5 times in case of failure before starting backend 
    
    - [IMPORTANT] study how the volumes' references have been set up inside postgresql, mongodb, and redis 
      services for persistent storage of data. 
      The left side of the colon denotes that on our local machine, Docker will create a named volume and manage the data in it. On the other hand, the right side of the colon denotes the path where the data resides inside the container itself.
      For example: in 'postgres_data:/var/lib/postgresql/data', in our local computer, the data will be stored in a volume called postgres_data somewhere; we don't need to worry about it since docker manages it. But inside the container itself, the data can be found at "/var/lib/postgresql/data".
    - When a launched docker container is stopped with docker-compose down or is interrupted, the data in
      these named columes persists because the named volumes are still in our local machine and data can be restored the next time the container is composed up.
    - Volumes can only be deleted explicitly with docker volume rm <volume_name>, not otherwise

  Lastly, notice how the global volumes fields at the botton defines all the local storage volumes postgres_data, mongodb_data, and redis_data
__________________________________________________________________________________________________________

5. NOTES ON the FastAPI backend [IMPORTANT]
  In backend.dockerfile, we run 
    CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
  This means the container looks for an app instance of the FastAPI in main.py within the app directory of the container, i.e. app.main (left of the colon) -> /app/main.py and app (right of the colon) ->
  app = FastAPI() instance within main.py

  Some Key Terms and Definitions for Python Backend Web Servers [IMPORTANT - FULL READ]:

    - Traditional Web Servers:
      - These serve static files, media, html, etc. (Example: Nginx, Apache, Caddy, LiteSpeed, etc.)
      - They do not run Python applications directly, but can forward requests to backend (WSGI/ASGI) servers
        and return the responses from those backend servers to the client
      - They also act as reverse proxy, i.e.
        - They provide load balancing by distributing requests across multiple WSGI/ASGI servers
        - They hide backend servers from direct public access, adding a layer of security
        - They handle static files, compression, and caching better than an application server
      - NGINX, APACHE, etc. are software packages that enable a computer to act as a web server, i.e., listen 
        for all incoming requests on port 80 (HTTP) or 443 (HTTPS) from any IP address and serve static files or forward the request to a backend application server and send responses back to the clients
    
    - Application/Backend Servers:
      - These are responsible for running dynamic web applications with dynamic API endpoints
      - They are of two types (in terms of Python application servers): WSGI and ASGI
      - Gateway Interface refers to the layer between the application server and the Python web application

        - WSGI server: Web Server Gateway Interface (WSGI) is an older standard for synchronous Python 
          web applications, that defines how a web server (like NGINX/Apche) communicates with a Python web application.
          Key Points:
          - Synchronous: It handles one request at a time, so each worker process gets blocked until it 
            handles its currently-being-processed request
          - Middleware Support: It allows the use of middleware like authentication, logging, etc. to 
            modify requests and responses
          - It is the de facto standard for Python web app frameworks like Django and Flask in synchronous mode
          - Examples: Gunicorn and uWSGI -> when we install and run Gunicorn or uWSGI, it turns our computer into
            a WSGI server, allowing us to serve Flask or Django apps
          - Some WSGI servers (like Gunicorn) support multiple worker processes, which allows handling multiple 
            requests concurrently, but each worker is still synchronous
          - Number of workers can be 2 x (number of CPU cores) + 1 maximum

          HOW A WSGI SERVER WORKS (Example):
            Let's say a user wants to add an item (a task) to a certain web application, and there is a Flask app that provides a route and a POST method for it -> "www.example-app.com/api/add-task/<task-info>". Assume that this method communicates with a database, adds the task into it, and responds with status 201 and a simple JSON response {"message":"task added"}. Then the steps involved are:
            - User/Client: Sends a POST request to www.example-app.com/api/add-task/<task-info>
            - Web Server (NGINX): Receives the HTTP request, acts as a reverse proxy, forwards it 
              to the appropriate backend application server based on the "/api/add-task/<task-info>" portion
            - Gunicorn: Is running and listening on port 8000 (say) and receives the request on the WSGI 
              server it's running on. It then matches the URL with the appropriate route defined in the Flask app
            - Flask App: Processes the request inside that method, extracts the task-info, interacts with 
              the database server to add the task and returns the response JSON to Gunicorn
            - Gunicorn: Sends the response back to the NGINX web server
            - Web Server (NGINX): Receives the response from Gunicorn and forwards it to the client
            - User/Client: Receives the response

        - ASGI server: ASGI (Asynchronous Server Gateway Interface) is a newer standard for asynchronous Python 
          web applications, introduced to support modern use cases like WebSockets and long-lived connections.
          Key Points:
          - Asynchronous: It can handle many requests concurrently using async I/O, ideal for WebSockets 
            and streaming APIs. Thus, ASGI can handle real-time communication
          - Designed for modern frameworks like FastAPI, Django Channels, and Starlette
          - Examples: Uvicorn and Daphne
          - ASGI servers like Uvicorn leverage async programming with event loops (like those in 
            Python's asyncio), making them more scalable for concurrent tasks.

          HOW An ASGI SERVER WORKS (Example):
          Let's say a user registers to a web application by providing their email and username. With that they create a POST request to "www.example-app.com/register/". Within the body of the POST request, say there is:
            {
              "username": "johndoe",
              "email":"johndoe@gmail.com"
            }
          As we saw in WSGI server, the usual behavior is for the NGINX to act as a reverse proxy and forward the request and response to and from between the client and uvicorn, which then matches the url to the specific method defined in the running FastAPI instance.
          - FastAPI: Receives the incoming request, validates the data using the User model (say), and 
            starts processing. However, instead of waiting for the confirmation email to be sent (which being a network I/O task can take a long time), the task gets added to the background and is run asynchronously. In the meantime, other requests or tasks are being handled by the FastAPI backend, effectively implementing non-blocking in the server. However, a response of status 201, and say a JSON response of {"message":"User registered!"} is sent immediately, again without waiting for the email to be sent. The email gets sent eventually. 
            Thus, "www.example-app.com/register/" endpoint receives the user data, saves it and responds immediately, while the network I/O task of sending a confirmation email is executed asynchronously in the background.
      
      - It looks like ASGI is much faster than WSGI. While, it is sort of true but context matters.
      - If we look at the number of requests handled per unit of time (say per second or per minute), ASGI
        can definitely handle lot more requests than WSGI. However, if we look at individual request handling time there may not be much of a difference. Also a lot depends on how the programmer writes the asynchronous code.
      - In the worst case scenario the asynchronous code may well take more time to handle a request than
        the synchronous code.
      - ASGI may also be able to handle I/O operations better than WSGI but when we look at CPU bound 
        operations, the difference may not be very high. 
      - Additionally, we may be able to spot the difference only during very high load. The advantage 
        of concurrency may not be visible at lesser load, taking all these into consideration.
      
      - ASGI is indeed the spiritual successor to WSGI as it has got the added benefit 
        of concurrency
          
  READ THIS AFTER THE ABOVE SECTION
  - uvicorn: an ASGI web server implementation for Python/FastAPI framework
__________________________________________________________________________________________________________
  
6. SOME BASIC TERMS BEFORE MOVING ON...
SQLAlchemy - ORM for PostgreSQL
  - Object Relational Mapper (ORM) is a tool that represents tables in databases through OOP classes and we
    can use OOP for performing CRUD operations instead of having to write actual SQL queries
Pydantic 
  - ensures data validation before sending it to the database and defines the structure of 
  incoming and outgoing data
Motor 
  - async MongoDB driver
__________________________________________________________________________________________________________

7. NOTES ON database.py (SQLALCHEMY's ROLE)
  from sqlalchemy import create_engine
  - This function creates a new engine, which is the starting point of any SQLAlchemy app
  - It establishes connectivity to our database

  from sqlalchemy.orm import sessionmaker [IMPORTANT]
  - Session Establishment and Purpose:
    - Establishes all conversations with the database
    - Acts as a “holding zone” for all objects loaded or associated with it during its lifespan
    - Provides the interface for executing SELECT and other queries that return or modify ORM-mapped objects.
    
  - Identity Map:
    - Maintains ORM objects within the Session in a structure called the identity map
    - Ensures uniqueness by keeping only one object per particular primary key

  - Transaction Lifecycle:
    - Begins in a mostly stateless form
    - Once queries are issued or objects are persisted, it requests a connection from an associated Engine.
    - Establishes a transaction on that connection which remains in effect until the Session is instructed 
      to commit or roll back

  - Change Tracking and the Unit of Work Pattern:
    - ORM objects are instrumented to generate change events whenever an attribute or collection is modified.
    - Before querying the database or committing a transaction, the Session flushes all pending changes (this 
      is the unit of work pattern)

  - ORM Object Behavior and Synchronization:
    - ORM-mapped objects are considered proxy objects to database rows, local to the Session’s transaction.
    - Various events trigger these objects to re-access the database to stay synchronized with the actual 
      database state

  - Detaching Objects:
    - It is possible to “detach” objects from a Session and continue using them, though this comes with caveats.
    - Typically, detached objects should be re-associated with another Session to resume their normal role 
      of representing database state
  
  READ THE FULL EXCERPT ABOVE BEFORE MOVING ON

  In database.py, in SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine), we see the 
  function parameters autocommit and autoflush
    - Commit (as in any database) finalizes the transaction to the database
    - Rollback (its opposite) negates/cancels any changes made during the transaction
    - Flushing means that synchronizing our session's temporary in-memory changes (inserts, updates, 
      deletes, etc.) that happen during transaction before finalizing, with the database by sending the appropriate SQL statements
    - autoflush=False makes it so that any and all changes is kept in sessionmaker's "hoarding zone" and 
      requiring explicit flush or commit from the user to synchronize and finalize the transactional changes respectively.

  get_db function
    - A dependency function that provides a new database session for each request.
    - In FastAPI route functions, it's included as a dependency (using Depends(get_db)), ensuring that 
      every request gets a fresh session.
    - yield keyword: Acts like a context manager; provides the session to our route function and then, 
      once the function completes, it ensures the session is closed.
      IT YIELDS THE DB TO THE CALLING FUNCTION
__________________________________________________________________________________________________________

8. NOTES ON config.py and schemas.py (PYDANTIC's & SQLALCHEMY's ROLE)
  In this project, Pydantic's role is two-fold, and they show up at different stages of our app's Lifecycle:

  Let's take an example to understand this. Let's say a user is creating a profile (registering) in TaskFlow app. That would look something like the frontend asking to fill up a form with the required fields (username, password, and email) and sending that data in a POST request as a JSON body. Then FastAPI would get the JSON data, interact with the database, add the user to the database and respond to the frontend with 201 status and a JSON such as:
  { 
    'id': <userID>,
    'username' : <username>,
    'email' : <email>,
    'created_at' : <registration_datetime>
  }
  Now, let's look at how Pydantic and SQLAlchemy are involved in this application in terms of this one task.

  I. config.py -> DURING APPLICATION STARTUP
    - Here, when we instantiate the Settings class, inheriting from Pydantic's BaseSettings, 
      Pydantic automatically loads and validates the environment variables from our .env file, which contains out PostgreSQL database's URL.
    - This happens right when our application starts up. If any required variable is missing or is of the 
      wrong type, Pydantic will raise an error immediately, preventing our application from running with an invalid configuration.
    - It looks like overkill for TaskFlow, but large-scale apps, with lots of environ,ment variables would 
      benefit from this.
    
    - So, Pydantic here, basically ensures that DATABASE_URL, MONGODB_URI, REDIS_URL, and SECRET_KEY are 
      all present and of the string type.
    - The Config class let's us define specific configurations, for example, we have defined an env_file for 
      the Settings Pydantic model that links the .env file path
  
  II. models.py and schemas.py -> DURING REQUEST HANDLING (USER REGISTRATION)

    - When the frontend sends the user information to register, FastAPI uses the Pydantic model defined in 
      schemas.py to ensure that the JSON data adheres to the User model defined in models.py
    - It will check if all the fields are present or not and if any field value is of the wrong type and 
      generate an error for FastAPI. However, simple conversions, such as if 25 was needed and the data has '25', Pydantic can perform the conversions for the backend
    - The validations are done based on the schemas defined in schemas.py
      - UserCreate is the schema model in this case
      - It inherits from UserBase, so it checks if string username and email have been provided
      - And since password is defined within UserCreate itself, it checks for a string password as well

    - One this validation (and conversion) is complete, the data is deserialized into the ORM based on 
      UserCreate schema, --> DESERIALIZATION
      and passed to the relevant CRUD function

    - Now, SQLAlchemy maps it into an ORM object, a user instance object of the User model defined in models.py
    - Any additional auto-generated info like id, and created_date in this case is added by SQLAlchemy itself
    - This is the INTERNAL DATABASE REPRESENTATION of the data, an ORM object with class attributes as 
      the respective fields for the database's columns
    - It then handles the persistence by inserting the new record into the PostgreSQL database. SQLAlchemy 
      manages the connection, builds the appropriate SQL queries, and commits the changes

    - Finally, once SQLAlchemy has committed the transaction, it returns the ORM object of the new user data
      contained in the database to the API (we have to make our code return whatever we want from SQLAlchemy)
    - Now, using the UserResponse schema model defined within schemas.py, FastAPI validates and serializes the
      response sent by SQLAlchemy
      - The ORM object is passed into UserResponse model
      - Thanks to the orm_mode=True setting in the Pydantic model, it automatically reads the fields from 
        the SQLAlchemy object and converts it into a JSON-friendly format --> SERIALIZATION
      - DECOUPLING [IMPORTANT]: Even though the response has all the user information, with internal fields 
        like password, the UserResponse schema model exposes only username, email (from UserBase) and id, and created_date
    
    - So, with this interaction and co-operation between Pydantic and SQLAlchemy, the internal 
      database representation has all the relevant fields like id, password, created_date and others, but not all of them are exposed over the API through decoupling.